# WebCache Explorer

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„Pythonç½‘ç»œç¼“å­˜æ¢ç´¢å’Œæœç´¢å·¥å…·ï¼Œæ”¯æŒæ‰¹é‡å¹¶å‘æŠ“å–ã€æ™ºèƒ½ç¼“å­˜ç®¡ç†å’Œå…¨æ–‡æœç´¢ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **æ‰¹é‡å¹¶å‘æŠ“å–**: ä½¿ç”¨ `requests` + `ThreadPoolExecutor` å®ç°é«˜æ•ˆå¹¶å‘æŠ“å–
- ğŸ’¾ **æ™ºèƒ½ç¼“å­˜ç®¡ç†**: è‡ªåŠ¨ç¼“å­˜æŠ“å–å†…å®¹ï¼Œç»´æŠ¤ç´¢å¼•æ–‡ä»¶ï¼Œæ”¯æŒé‡å¤æŠ“å–
- ğŸ” **å…¨æ–‡æœç´¢**: åŸºäºå…³é”®è¯çš„æ™ºèƒ½æœç´¢ï¼Œè¿”å›åŒ¹é…åº¦æœ€é«˜çš„ç»“æœ
- ğŸ“Š **æ€§èƒ½ç»Ÿè®¡**: è¯¦ç»†çš„æŠ“å–æ€§èƒ½ç»Ÿè®¡å’Œç¼“å­˜ä½¿ç”¨æƒ…å†µ
- âš™ï¸ **çµæ´»é…ç½®**: æ”¯æŒé…ç½®æ–‡ä»¶ï¼Œå¯è‡ªå®šä¹‰å¹¶å‘æ•°ã€è¶…æ—¶ã€é‡è¯•ç­‰å‚æ•°
- ğŸ–¥ï¸ **å‘½ä»¤è¡Œç•Œé¢**: æä¾›å®Œæ•´çš„CLIå­å‘½ä»¤ï¼Œæ“ä½œç®€å•ç›´è§‚
- ğŸ§ª **å…¨é¢æµ‹è¯•**: åŒ…å«å®Œæ•´çš„pytestæµ‹è¯•å¥—ä»¶
- ğŸ“ˆ **æ€§èƒ½åŸºå‡†**: å†…ç½®æ€§èƒ½æµ‹è¯•è„šæœ¬ï¼Œè¯„ä¼°ç³»ç»Ÿæ€§èƒ½

## å®‰è£…

### ç³»ç»Ÿè¦æ±‚

- Python 3.7+
- Windows/Linux/macOS

### å¿«é€Ÿå®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-username/webcache_explorer.git
cd webcache_explorer

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
source venv/bin/activate  # Linux/macOS
# æˆ–
venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¼€å‘æ¨¡å¼å®‰è£…
pip install -e .
```

### å¼€å‘å®‰è£…

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# å®‰è£…é¢„æäº¤é’©å­ï¼ˆå¯é€‰ï¼‰
pre-commit install
```

## å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡URLåˆ—è¡¨

åˆ›å»º `urls.txt` æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªURLï¼š

```
https://httpbin.org/html
https://httpbin.org/json
https://example.com
https://python.org
```

### 2. æ‰¹é‡æŠ“å–

```bash
# åŸºæœ¬æŠ“å–
webcache_explorer fetch --urls urls.txt

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
webcache_explorer fetch --urls urls.txt --config config.toml

# å¼ºåˆ¶é‡æ–°æŠ“å–ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
webcache_explorer refetch --urls urls.txt
```

### 3. æ·»åŠ å•ä¸ªURL

```bash
# æ·»åŠ å•ä¸ªURLåˆ°ç¼“å­˜
webcache_explorer add-url https://example.com
```

### 4. æœç´¢å†…å®¹

```bash
# æœç´¢å…³é”®è¯
webcache_explorer search "python programming"

# æœç´¢å¹¶é™åˆ¶ç»“æœæ•°é‡
webcache_explorer search "web development" --top-k 5
```

### 5. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯

```bash
# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
webcache_explorer stats

# æŸ¥çœ‹ç‰¹å®šURLçš„è¯¦ç»†ä¿¡æ¯
webcache_explorer show https://example.com
```

### 6. å¯¼å‡ºæ•°æ®

```bash
# å¯¼å‡ºç¼“å­˜ç´¢å¼•
webcache_explorer export cache_export.json
```

## é…ç½®è¯´æ˜

### é…ç½®æ–‡ä»¶ (config.toml)

```toml
[fetching]
max_workers = 4          # æœ€å¤§å¹¶å‘æ•°
timeout = 30             # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
max_retries = 3          # æœ€å¤§é‡è¯•æ¬¡æ•°
retry_delay = 1.0        # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

[storage]
data_dir = "data"        # æ•°æ®ç›®å½•
index_file = "index.json" # ç´¢å¼•æ–‡ä»¶å

[processing]
max_content_size = 10485760  # æœ€å¤§å†…å®¹å¤§å°ï¼ˆå­—èŠ‚ï¼‰

[logging]
level = "INFO"           # æ—¥å¿—çº§åˆ«
file = "webcache_explorer.log"  # æ—¥å¿—æ–‡ä»¶
```

### ç¯å¢ƒå˜é‡

- `WEBCACHE_CONFIG`: é…ç½®æ–‡ä»¶è·¯å¾„
- `WEBCACHE_DATA_DIR`: æ•°æ®ç›®å½•è·¯å¾„

## æ€§èƒ½æµ‹è¯•

### è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•
python bench.py

# è¿è¡Œpytestæµ‹è¯•å¥—ä»¶
pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_crawler.py

# ç”Ÿæˆæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=webcache_explorer --cov-report=html
```

### æ€§èƒ½æŒ‡æ ‡

åŸºå‡†æµ‹è¯•ä¼šæµ‹é‡ä»¥ä¸‹æŒ‡æ ‡ï¼š

- å•URLæŠ“å–æ€§èƒ½
- å¹¶å‘æŠ“å–æ€§èƒ½
- ç¼“å­˜å­˜å‚¨å’Œæ£€ç´¢æ€§èƒ½
- æˆåŠŸç‡ç»Ÿè®¡
- å†…å®¹å¤§å°åˆ†æ
- è¯·æ±‚ååé‡

## é¡¹ç›®ç»“æ„

```
webcache_explorer/
â”œâ”€â”€ src/webcache_explorer/    # ä¸»è¦æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py           # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ config.py             # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ crawler.py            # ç½‘ç»œçˆ¬è™«
â”‚   â”œâ”€â”€ cache.py              # ç¼“å­˜ç®¡ç†
â”‚   â”œâ”€â”€ text_processor.py     # æ–‡æœ¬å¤„ç†å’Œæœç´¢
â”‚   â””â”€â”€ cli.py                # å‘½ä»¤è¡Œæ¥å£
â”œâ”€â”€ tests/                    # æµ‹è¯•å¥—ä»¶
â”‚   â”œâ”€â”€ test_config.py        # é…ç½®æµ‹è¯•
â”‚   â”œâ”€â”€ test_crawler.py       # çˆ¬è™«æµ‹è¯•
â”‚   â”œâ”€â”€ test_cache.py         # ç¼“å­˜æµ‹è¯•
â”‚   â”œâ”€â”€ test_text_processor.py  # æ–‡æœ¬å¤„ç†æµ‹è¯•
â”‚   â””â”€â”€ test_cli.py           # CLIæµ‹è¯•
â”œâ”€â”€ config/                   # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ config.toml           # é»˜è®¤é…ç½®
â”œâ”€â”€ data/                     # æ•°æ®ç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ index.json            # ç¼“å­˜ç´¢å¼•
â”‚   â””â”€â”€ content/              # ç¼“å­˜å†…å®¹
â”œâ”€â”€ requirements.txt          # ç”Ÿäº§ä¾èµ–
â”œâ”€â”€ requirements-dev.txt      # å¼€å‘ä¾èµ–
â”œâ”€â”€ pyproject.toml            # é¡¹ç›®å…ƒæ•°æ®
â”œâ”€â”€ bench.py                  # æ€§èƒ½æµ‹è¯•è„šæœ¬
â”œâ”€â”€ urls.txt                  # ç¤ºä¾‹URLåˆ—è¡¨
â””â”€â”€ README.md                 # é¡¹ç›®æ–‡æ¡£
```

## API ä½¿ç”¨

### ä½œä¸ºåº“ä½¿ç”¨

```python
from webcache_explorer import Config, WebCrawler, CacheManager, TextProcessor

# åˆ›å»ºé…ç½®
config = Config()

# åˆå§‹åŒ–ç»„ä»¶
crawler = WebCrawler(config)
cache_manager = CacheManager(config)
text_processor = TextProcessor()

# æŠ“å–URL
result = crawler.fetch_url("https://example.com")

# å­˜å‚¨åˆ°ç¼“å­˜
cache_manager.store(result)

# æœç´¢å†…å®¹
entries = cache_manager.get_successful_urls()
cache_entries = [cache_manager.retrieve(url) for url in entries]
results = text_processor.search_content(cache_entries, "search term")

# æ‰“å°ç»“æœ
for result in results:
    print(f"URL: {result.url}")
    print(f"Title: {result.title}")
    print(f"Score: {result.relevance_score}")
    print(f"Summary: {result.summary}")
    print("-" * 50)
```

## é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰æœç´¢ç®—æ³•

```python
from webcache_explorer.text_processor import TextProcessor

processor = TextProcessor()

# è‡ªå®šä¹‰ç›¸å…³æ€§è¯„åˆ†
score = processor.calculate_relevance_score(
    text="Python programming tutorial",
    query="python tutorial"
)

# æå–å…³é”®è¯
keywords = processor.extract_keywords(
    text="Python is a programming language for web development",
    top_k=5
)

# ç”Ÿæˆæ‘˜è¦
summary = processor.generate_summary(
    text="Long article content...",
    max_sentences=3
)
```

### æ‰¹é‡å¤„ç†

```python
from webcache_explorer import WebCrawler

crawler = WebCrawler()

# æ‰¹é‡æŠ“å–URLs
urls = [
    "https://example1.com",
    "https://example2.com",
    "https://example3.com"
]

results = crawler.fetch_urls(urls)

# å¤„ç†ç»“æœ
for result in results:
    if result['success']:
        print(f"âœ“ {result['url']}: {result['status_code']}")
    else:
        print(f"âœ— {result['url']}: {result.get('error_message')}")
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿æ¥è¶…æ—¶**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - å¢åŠ é…ç½®æ–‡ä»¶ä¸­çš„ `timeout` å€¼
   - æ£€æŸ¥ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®

2. **å†…å­˜ä¸è¶³**
   - å‡å°‘ `max_workers` å¹¶å‘æ•°
   - å‡å° `max_content_size` é™åˆ¶
   - åˆ†æ‰¹å¤„ç†å¤§é‡URL

3. **æƒé™é”™è¯¯**
   - ç¡®ä¿æœ‰å†™å…¥æ•°æ®ç›®å½•çš„æƒé™
   - æ£€æŸ¥é…ç½®æ–‡ä»¶æƒé™

4. **SSLè¯ä¹¦é”™è¯¯**
   - æŸäº›ç½‘ç«™å¯èƒ½éœ€è¦ç‰¹æ®ŠSSLé…ç½®
   - è€ƒè™‘ä½¿ç”¨ä»£ç†æˆ–VPN

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è°ƒè¯•æ—¥å¿—
export WEBCACHE_LOG_LEVEL=DEBUG
webcache_explorer fetch --urls urls.txt

# æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶
# åœ¨ config.toml ä¸­è®¾ç½® logging.level = "DEBUG"
```

## è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»º Pull Request

### å¼€å‘è§„èŒƒ

- éµå¾ª PEP 8 ä»£ç é£æ ¼
- æ·»åŠ ç±»å‹æ³¨è§£
- ç¼–å†™å•å…ƒæµ‹è¯•
- æ›´æ–°æ–‡æ¡£
- ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## æ›´æ–°æ—¥å¿—

### v1.0.0 (2024-01)
- âœ¨ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- ğŸš€ æ‰¹é‡å¹¶å‘æŠ“å–åŠŸèƒ½
- ğŸ’¾ æ™ºèƒ½ç¼“å­˜ç®¡ç†
- ğŸ” å…¨æ–‡æœç´¢åŠŸèƒ½
- ğŸ“Š æ€§èƒ½ç»Ÿè®¡å’ŒåŸºå‡†æµ‹è¯•
- ğŸ–¥ï¸ å®Œæ•´çš„CLIç•Œé¢
- ğŸ§ª å…¨é¢çš„æµ‹è¯•å¥—ä»¶

## è”ç³»æ–¹å¼

- é¡¹ç›®ä¸»é¡µ: https://github.com/your-username/webcache_explorer
- é—®é¢˜åé¦ˆ: https://github.com/your-username/webcache_explorer/issues
- é‚®ç®±: your.email@example.com

---

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼